# â›“ï¸ SCD Type 2 Data Engineering Pipeline with Snowflake + S3 + Faker

This project showcases a complete **data engineering pipeline** that simulates handling Slowly Changing Dimensions (SCD Type 1 & 2) using **Snowflake**, **Amazon S3**, and **Python Faker**. It mimics a real-world use case where customer information changes over time and needs to be tracked historically.

## ğŸ§± Tech Stack

- **Snowflake** â€” Cloud Data Warehouse
- **Amazon S3** â€” Staging Layer for raw files
- **Faker + Python** â€” Data simulation
- **Snowpipe** â€” Automated data ingestion
- **Streams** â€” Change data capture
- **Docker** â€” Deployment simulation
- **SQL (SCD1 & SCD2)** â€” Data transformation logic

---

## ğŸ“Œ Project Highlights

- ğŸ” **SCD Type 2** history tracking: preserves old data with timestamps.
- ğŸ’¡ Uses **Snowflake Stream** to capture changes from a staging table.
- ğŸ“‚ Loads raw files from **S3** using Snowpipe and external stage.
- ğŸ§ª Faker generates dynamic test data mimicking real customers.
- ğŸ³ Docker Compose used for version-controlled environment simulation.

---

## ğŸ“‚ Folder Structure

- `sql/` â€” All SQL scripts for table creation, data loading, and SCD transformations.
- `faker.ipynb` â€” Notebook that generates fake customer data and pushes to S3.
- `architecture/` â€” Project pipeline diagram.
- `docker-compose.yml` â€” Sets up supporting services (if any in real scenario).
- `commands.txt` â€” Helpful CLI and Snowflake commands.

---

## ğŸ“Š Architecture

![Architecture](architecture/scd_pipeline_architecture.png)

---

## ğŸš€ Run the Pipeline

### 1. Create Snowflake Objects
```sql
-- Run sql/1_table_creation.sql
-- Run sql/2_data_loading.sql to create stages, file formats, and pipes
```

### 2. Run Data Generator
```bash
jupyter notebook notebooks/faker.ipynb
# Or run as script to generate fake customer records into S3
```

### 3. Monitor Snowpipe
```sql
SELECT SYSTEM$PIPE_STATUS('customer_s3_pipe');
SELECT * FROM customer_raw;
```

### 4. Apply Transformations
```sql
-- Run sql/3_scd1.sql or sql/4_scd2.sql depending on scenario
```

---

## ğŸ“š Learning Outcomes

- Understand the difference between SCD1 and SCD2.
- Automate data ingestion using Snowpipe.
- Use Python to simulate real-world ETL testing environments.
- Apply Snowflake streams to detect and apply changes.

---

## ğŸªª License

This project is for learning and demonstration purposes only. Feel free to fork and extend!
